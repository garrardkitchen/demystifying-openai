<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">

		<style>
			/* any selector under .reveal .slides will be scoped to your slides */
			.reveal .slides .red-text { color: #e74c3c; }
			.reveal .slides .blue-text { color: #3498db; }
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>

					<textarea data-template>
						## demystifying (open)AI
				
						<span class="blue-text">_The Basics_</span>
					</textarea>

				</section>
			
				<!-- <section>
					<h3>Agenda</h3>
					<div class="mermaid">
					  <pre>
						%%{init: {'theme': 'dark', 'themeVariables': { 'darkMode': true }}}%%
						
						kanban
							todo[Todo]
							  id4[HLD: Overview]@{ ticket: DM-0002, assigned: 'gpk', priority: 'High' }
							 
							term[In-progress]
								id3[Terminology]@{ ticket: DM-0001, assigned: 'gpk', priority: 'High' }
							 
							demo[Done]
								id3_1[RAG: Console app]@{ ticket: DM-0013, assigned: 'gpk', priority: 'High' }					
								id3_2[MCP: VSCode Hosting]@{ ticket: DM-0014, assigned: 'gpk', priority: 'High' }
								id3_3[Chat: Web app]@{ ticket: DM-0014, assigned: 'gpk', priority: 'High' }
					  </pre>
					</div>
				</section> -->
				<section>
					<h6>HLD</h6>
					<div class="mermaid">
					  <pre>
						%%{init: {'theme': 'dark', 'themeVariables': { 'darkMode': true }}}%%
						
						
						flowchart LR
							classDef blue fill:#336699,color:#fff
							classDef orange fill:#FFA500,color:#000
							classDef green fill:#4CAF50,color:#333
							classDef gray fill:#808080,color:#333
							classDef yellow fill:#FFFF00,color:#333
							classDef red fill:#FF0000,color:#fff
							classDef purple fill:#800080,color:#fff
							classDef sub fill:#000,color:#fff, stroke:#fff, stroke-width:1px, stroke-dasharray: 5 5, padding: 2px, font-weight: bold

							user_input@{ shape: sl-rect, label: "User request" }
							llm@{ shape: processes, label: "LLM" }
							vector@{ shape: cyl, label: "Vector Store" }
							docs@{ shape: lin-doc, label: "PDFs" }
							data@{ shape: cyl, label: "Corp Db" }
							embeddings@{ shape: sl-rect, label: "Embedding" }
							semantic_search@{ shape: odd, label: "Semantic Search" }

							chunking@{ shape: docs, label: "Chunking" }
							indexing@{ shape: notch-rect, label: "Indexing" }
							
							retrieval@{ shape: hex, label: "Retrieval" }
							augmentation@{ shape: doc, label: "Augmentation" }
							generation@{ shape: stadium, label: "Generation" }

							tools@{ shape: hex, label: "Tools" }
							documents@{ shape: docs, label: "Documents" }
							db@{ shape: cyl, label: "Databases" }
							api@{ shape: processes, label: "APIs" }


							user_input:::blue --> llm
							user_input --> semantic_search
							semantic_search:::red --> retrieval
							embeddings:::green <---> retrieval:::orange
							retrieval --> augmentation:::yellow
							augmentation --> generation:::yellow
							generation --> llm:::green
							vector:::orange --> retrieval

							RAG:::sub
							Ingestion:::sub
							MCP:::sub
							
							subgraph RAG
								retrieval
								augmentation
								generation
							end

							subgraph Ingestion
								docs:::purple --> chunking:::orange
								data:::purple --> chunking
								chunking --> indexing:::orange
								indexing --> embeddings
								embeddings --> vector:::orange
							end

							user_input --> tools
							tools:::orange <---> llm
							
							subgraph MCP

								tools <---> api:::purple
								tools <---> db:::purple
								tools <----> documents:::purple

							end
					  </pre>
					</div>
				</section>
				<section data-markdown>
					
					<textarea data-template>

						## Terminology

						---

						Definition of **Ingestion**

						> The process of collecting and preparing data (such as documents or database records) for use in an AI system, often involving cleaning and formatting.

						---

						Definition of **Chunking**

						> The process of splitting large documents or datasets into smaller, manageable pieces (chunks) to facilitate processing, embedding, and retrieval.

						---

						Definition of **Indexing**

						> Creating a <span class="blue-text">searchable structure</span> (index) from data chunks, enabling <span class="blue-text">efficient</span> lookup and <span class="blue-text">retrieval</span> during semantic search.

						---

						Definition of **Embeddings**

						> <span class="blue-text">Numerical vector representations of text or data</span>, capturing <span class="blue-text">semantic meaning</span> and enabling similarity comparisons in vector space.

						---

						Definition of **Vector store**

						> A specialized database for <span class="blue-text">storing and searching embeddings</span>	 (vectors), supporting fast similarity search and retrieval.

						---

						Definition of **Caching (CAG)**

						> The practice of storing frequently accessed or computed results (such as embeddings or search results) to speed up future operations, <span class="blue-text">reduce redundant computation</span> and <span class="blue-text">token usage</span>.

						---

						Definition of **Semantic search**

						> A search technique that uses embeddings to find information based on <span class="blue-text">meaning</span> and <span class="blue-text">context</span>, rather than exact keyword matches.

						---

						Definition of **Retrieval**

						> The process of finding and returning <span class="blue-text">relevant data chunks or documents</span> from a vector store or index, typically using <span class="blue-text">semantic search</span>.

						---

						Definition of **Augmentation**

						> Enhancing or <span class="blue-text">enriching a user query or prompt</span> with retrieved context or <span class="blue-text">external information</span> before passing it to a language model.

						---

						Definition of **Generation**

						> The process where a language model (LLM) produces new text or responses <span class="blue-text">based on the augmented prompt</span> and context.

						---

						Definition of **Rating**

						> Evaluating or scoring the quality, relevance, or accuracy of generated responses or retrieved results, often for feedback or <span class="blue-text">improvement</span>.

						---

						Definition of **Token**

						> The smallest <span class="blue-text">unit of text</span> (such as a word piece, subword, or character) that a language model processes; tokens are used to measure input length, control context windows, and <span class="blue-text">calculate usage</span>.

						---

						Definition of **Tokenization**
						
						> The process of <span class="blue-text">breaking text into tokens</span> according to a modelâ€™s vocabulary and rules; tokenization transforms raw text into a sequence of tokens that can be <span class="blue-text">fed into an LLM</span> for processing.
						
			
						---

						**Tokens** and **Tokenization**

						> <span class="red-text">Tokenization</span> is the <span class="red-text">**method**</span>, and <span class="blue-text">tokens</span> are the <span class="blue-text">**output**</span> of that method.
						
						---

						Definition of **LLM**

						> Large Language Model; an advanced AI model trained on vast amounts of text data designed to understand, generate, and process human language.

						---

						Definition of **MCP**

						> Model Context Protocol; an open protocol that standardizes how applications provide context to LLMs, and provides a <span class="blue-text">standardized</span> way to connect AI models to different <span class="blue-text">data sources</span> and <span class="blue-text">tools</span>.

						
					</textarea>
					
				</section>


				<section data-markdown>
					
					<textarea data-template>

						A code example: The RAG pattern

						<pre><code data-trim class="language-csharp" data-line-numbers="3-5|6|9-12|14-20|23|25|27|29" data-ln-start-from="7">
						public static async Task TextEmbedding()
						{
							var openAiClient = CreateOpenAiClient();
							var chatClient = openAiClient.AsChatClient("gpt-4o-mini");
							var generator = openAiClient.GetEmbeddingClient("text-embedding-3-small").AsEmbeddingGenerator();
							var collection = await GetCollectionCreateIfNotExistsAsync();
							
							// user query
							var query = "I want a pleasant short story about a drupert. " +
										"This story is to be less than 5 sentences. " +
										"It has to make me want to laugh out loud. It must end with a hugging emoji " +
										"and this emogi has to be on a new line. The story must have a title";
					
							string[] externalData = [
								"A Drupert is a fictional creature", 
								"A Sleepert is meant to make you fall asleep during school time", 
								"Chicken Jockie!",
								"A Drupert is meant to distract", 
								"A Drupert sole purpose is to make you laugh", 
								"If someone draws a Drupert during class time and you laugh, you'll likely be told off my your teacher"];
							
							// step 1: ingestion (chunking & indexing). Tokenization
							await IngestionAsync(collection, generator, externalData);
							// step 2: retrieval. Tokenization
							var searchResults = await SemanticSearchAsync(collection, generator, query);
							// step 3: augmentation
							var prompt = CreatePrompt(searchResults.Results, query);
							// step 4: generation. Tokenization
							Console.WriteLine(await chatClient.GetResponseAsync(new ChatMessage(ChatRole.User, prompt)));
						}
						</code></pre>
					</textarea>
				</section>

				
				<section>Demo Time!</section>
				<section>
				<section>Demo 1: RAG</section>
				<section>Demo 2: MCP</section>
				</section>

				<!-- <section>
				<section data-transition="zoom">
					<h2>This slide will override the presentation transition and zoom!</h2>
				  </section>
				  
				  <section data-transition-speed="fast">
					<h2>Choose from three transition speeds: default, fast or slow!</h2>
				  </section>
				</section> -->

				<section>Thanks for watching!</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@11.4.1/plugin/mermaid/mermaid.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				width: 900,
				height: 500,
				  // mermaid initialize config
				  mermaid: {
					// flowchart: {
					//   curve: 'linear',
					// },
					},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMermaid ]
			});
		</script>
	</body>
</html>
